{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named pysqlite3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f0c9556489fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0murlparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murljoin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpysqlite3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdbapi2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msqlite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named pysqlite3"
     ]
    }
   ],
   "source": [
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "from urlparse import urljoin\n",
    "from pysqlite3 import dbapi2 as sqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ignorewords = set(['the','of','to','and','a','in','is','it'])\n",
    "\n",
    "class crawler: \n",
    "# Initialize the crawler with the name of database def __init__(self,dbname):\n",
    "    def __init__(self,dbname):\n",
    "        self.con=sqlite.connect(dbname)\n",
    "    \n",
    "    def __del__(self): \n",
    "        self.con.close()\n",
    "    \n",
    "    def dbcommit(self):\n",
    "        self.con.commit()\n",
    "# Auxilliary function for getting an entry id and adding\n",
    "    # it if it's not present\n",
    "    def getentryid(self,table,field,value,createnew=True):\n",
    "        cur=self.con.execute(\"select rowid from %s where %s='%s'\" % (table,field,value))\n",
    "        res=cur.fetchone()\n",
    "        if res==None:\n",
    "            cur=self.con.execute(\"insert into %s (%s) values ('%s')\" % (table,field,value))\n",
    "            return cur.lastrowid\n",
    "        else:\n",
    "            return res[0]\n",
    "        \n",
    "       # Index an individual page\n",
    "    def addtoindex(self,url,soup):\n",
    "        if self.isindexed(url): \n",
    "            return\n",
    "        print 'Indexing '+url\n",
    "  \n",
    "        # Get the individual words\n",
    "        text=self.gettextonly(soup)\n",
    "        words=self.separatewords(text)\n",
    "    \n",
    "        # Get the URL id\n",
    "        urlid=self.getentryid('urllist','url',url)\n",
    "    \n",
    "    # Link each word to this url\n",
    "        for i in range(len(words)):\n",
    "            word=words[i]\n",
    "            if word in ignorewords: \n",
    "                continue\n",
    "            wordid=self.getentryid('wordlist','word',word)\n",
    "            self.con.execute(\"insert into wordlocation(urlid,wordid,location) values (%d,%d,%d)\" % (urlid,wordid,i))\n",
    "    \n",
    "    # Extract the text from an HTML page (no tags)\n",
    "    def gettextonly(self,soup):\n",
    "        v=soup.string\n",
    "        if v==Null:   \n",
    "            c=soup.contents\n",
    "            resulttext=''\n",
    "            for t in c:\n",
    "                subtext=self.gettextonly(t)\n",
    "                resulttext+=subtext+'\\n'\n",
    "            return resulttext\n",
    "        else:\n",
    "            return v.strip()\n",
    "        \n",
    "       # Separate the words by any non-whitespace character\n",
    "    def separatewords(self,text):\n",
    "        splitter=re.compile('\\\\W*')\n",
    "        return [s.lower() for s in splitter.split(text) if s!='']\n",
    "\n",
    "    # Return true if this url is already indexed\n",
    "    def isindexed(self,url):\n",
    "        return False\n",
    "      \n",
    "    # Add a link between two pages\n",
    "    def addlinkref(self,urlFrom,urlTo,linkText):\n",
    "        words=self.separatewords(linkText)\n",
    "        fromid=self.getentryid('urllist','url',urlFrom)\n",
    "        toid=self.getentryid('urllist','url',urlTo)\n",
    "        if fromid==toid: return\n",
    "        cur=self.con.execute(\"insert into link(fromid,toid) values (%d,%d)\" % (fromid,toid))\n",
    "        linkid=cur.lastrowid\n",
    "        for word in words:\n",
    "            if word in ignorewords: continue\n",
    "            wordid=self.getentryid('wordlist','word',word)\n",
    "            self.con.execute(\"insert into linkwords(linkid,wordid) values (%d,%d)\" % (linkid,wordid))\n",
    "      # Starting with a list of pages, do a breadth\n",
    "      # first search to the given depth, indexing pages\n",
    "      # as we go\n",
    "    \n",
    "    def crawl(self,pages,depth=2):\n",
    "        for i in range(depth):\n",
    "            newpages = set()\n",
    "            for page in pages:\n",
    "                try:\n",
    "                    c = urllib2.urlopen(page)\n",
    "                except:\n",
    "                    print \"could not open %s\" % page\n",
    "                    continue\n",
    "                soup = BeautifulSoup(c.read())\n",
    "                self.addtoindex(page,soup)\n",
    "                \n",
    "                links = soup('a')\n",
    "                for link in links:\n",
    "                    if ('href' in dict(link.attrs)):\n",
    "                        url = urljoin(page,link['href'])\n",
    "                        if url.find(\"'\") != -1:\n",
    "                            continue\n",
    "                        url = url.split('#'[0])\n",
    "                        if url[0:4]=='http' and not self.isindexed(url):\n",
    "                            newpages.add(url)\n",
    "                        linkText = self.gettextonly(link)\n",
    "                        self.addlinkref(page, url, linkText)\n",
    "                        \n",
    "                self.dbcommit()\n",
    "                \n",
    "            page = newpages\n",
    "                        \n",
    "      # Create the database tables\n",
    "    def createindextables(self): \n",
    "        self.con.execute('create table urllist(url)')\n",
    "        self.con.execute('create table wordlist(word)')\n",
    "        self.con.execute('create table wordlocation(urlid,wordid,location)')\n",
    "        self.con.execute('create table link(fromid integer,toid integer)')\n",
    "        self.con.execute('create table linkwords(wordid,linkid)')\n",
    "        self.con.execute('create index wordidx on wordlist(word)')\n",
    "        self.con.execute('create index urlidx on urllist(url)')\n",
    "        self.con.execute('create index wordurlidx on wordlocation(wordid)')\n",
    "        self.con.execute('create index urltoidx on link(toid)')\n",
    "        self.con.execute('create index urlfromidx on link(fromid)')\n",
    "        self.dbcommit()\n",
    "\n",
    "    def calculatepagerank(self,iterations=20):\n",
    "        # clear out the current page rank tables\n",
    "        self.con.execute('drop table if exists pagerank')\n",
    "        self.con.execute('create table pagerank(urlid primary key,score)')\n",
    "    \n",
    "    # initialize every url with a page rank of 1\n",
    "        for (urlid,) in self.con.execute('select rowid from urllist'):\n",
    "            self.con.execute('insert into pagerank(urlid,score) values (%d,1.0)' % urlid)\n",
    "        self.dbcommit()\n",
    "    \n",
    "        for i in range(iterations):\n",
    "            print \"Iteration %d\" % (i)\n",
    "            for (urlid,) in self.con.execute('select rowid from urllist'):\n",
    "                pr=0.15\n",
    "        \n",
    "        # Loop through all the pages that link to this one\n",
    "        for (linker,) in self.con.execute(\n",
    "        'select distinct fromid from link where toid=%d' % urlid):\n",
    "          # Get the page rank of the linker\n",
    "          linkingpr=self.con.execute(\n",
    "          'select score from pagerank where urlid=%d' % linker).fetchone()[0]\n",
    "\n",
    "          # Get the total number of links from the linker\n",
    "          linkingcount=self.con.execute(\n",
    "          'select count(*) from link where fromid=%d' % linker).fetchone()[0]\n",
    "          pr+=0.85*(linkingpr/linkingcount)\n",
    "        self.con.execute(\n",
    "        'update pagerank set score=%f where urlid=%d' % (pr,urlid))\n",
    "      self.dbcommit()\n",
    "\n",
    "class searcher:\n",
    "    def __init__(self,dbname):\n",
    "        self.con=sqlite.connect(dbname)\n",
    "\n",
    "    def __del__(self):\n",
    "        self.con.close()\n",
    "\n",
    "    def getmatchrows(self,q):\n",
    "        # Strings to build the query\n",
    "        fieldlist='w0.urlid'\n",
    "        tablelist=''  \n",
    "        clauselist=''\n",
    "        wordids=[]\n",
    "\n",
    "    # Split the words by spaces\n",
    "    words=q.split(' ')  \n",
    "    tablenumber=0\n",
    "\n",
    "    for word in words:\n",
    "      # Get the word ID\n",
    "      wordrow=self.con.execute(\n",
    "      \"select rowid from wordlist where word='%s'\" % word).fetchone()\n",
    "      if wordrow!=None:\n",
    "        wordid=wordrow[0]\n",
    "        wordids.append(wordid)\n",
    "        if tablenumber>0:\n",
    "          tablelist+=','\n",
    "          clauselist+=' and '\n",
    "          clauselist+='w%d.urlid=w%d.urlid and ' % (tablenumber-1,tablenumber)\n",
    "        fieldlist+=',w%d.location' % tablenumber\n",
    "        tablelist+='wordlocation w%d' % tablenumber      \n",
    "        clauselist+='w%d.wordid=%d' % (tablenumber,wordid)\n",
    "        tablenumber+=1\n",
    "\n",
    "    # Create the query from the separate parts\n",
    "    fullquery='select %s from %s where %s' % (fieldlist,tablelist,clauselist)\n",
    "    print fullquery\n",
    "    cur=self.con.execute(fullquery)\n",
    "    rows=[row for row in cur]\n",
    "\n",
    "    return rows,wordids\n",
    "\n",
    "  def getscoredlist(self,rows,wordids):\n",
    "    totalscores=dict([(row[0],0) for row in rows])\n",
    "\n",
    "    # This is where we'll put our scoring functions\n",
    "    weights=[(1.0,self.locationscore(rows)), \n",
    "             (1.0,self.frequencyscore(rows)),\n",
    "             (1.0,self.pagerankscore(rows)),\n",
    "             (1.0,self.linktextscore(rows,wordids)),\n",
    "             (5.0,self.nnscore(rows,wordids))]\n",
    "    for (weight,scores) in weights:\n",
    "      for url in totalscores:\n",
    "        totalscores[url]+=weight*scores[url]\n",
    "\n",
    "    return totalscores\n",
    "\n",
    "  def geturlname(self,id):\n",
    "    return self.con.execute(\n",
    "    \"select url from urllist where rowid=%d\" % id).fetchone()[0]\n",
    "\n",
    "  def query(self,q):\n",
    "    rows,wordids=self.getmatchrows(q)\n",
    "    scores=self.getscoredlist(rows,wordids)\n",
    "    rankedscores=[(score,url) for (url,score) in scores.items()]\n",
    "    rankedscores.sort()\n",
    "    rankedscores.reverse()\n",
    "    for (score,urlid) in rankedscores[0:10]:\n",
    "      print '%f\\t%s' % (score,self.geturlname(urlid))\n",
    "    return wordids,[r[1] for r in rankedscores[0:10]]\n",
    "\n",
    "  def normalizescores(self,scores,smallIsBetter=0):\n",
    "    vsmall=0.00001 # Avoid division by zero errors\n",
    "    if smallIsBetter:\n",
    "      minscore=min(scores.values())\n",
    "      return dict([(u,float(minscore)/max(vsmall,l)) for (u,l) in scores.items()])\n",
    "    else:\n",
    "      maxscore=max(scores.values())\n",
    "      if maxscore==0: maxscore=vsmall\n",
    "      return dict([(u,float(c)/maxscore) for (u,c) in scores.items()])\n",
    "\n",
    "  def frequencyscore(self,rows):\n",
    "    counts=dict([(row[0],0) for row in rows])\n",
    "    for row in rows: counts[row[0]]+=1\n",
    "    return self.normalizescores(counts)\n",
    "\n",
    "  def locationscore(self,rows):\n",
    "    locations=dict([(row[0],1000000) for row in rows])\n",
    "    for row in rows:\n",
    "      loc=sum(row[1:])\n",
    "      if loc<locations[row[0]]: locations[row[0]]=loc\n",
    "    \n",
    "    return self.normalizescores(locations,smallIsBetter=1)\n",
    "\n",
    "  def distancescore(self,rows):\n",
    "    # If there's only one word, everyone wins!\n",
    "    if len(rows[0])<=2: return dict([(row[0],1.0) for row in rows])\n",
    "\n",
    "    # Initialize the dictionary with large values\n",
    "    mindistance=dict([(row[0],1000000) for row in rows])\n",
    "\n",
    "    for row in rows:\n",
    "      dist=sum([abs(row[i]-row[i-1]) for i in range(2,len(row))])\n",
    "      if dist<mindistance[row[0]]: mindistance[row[0]]=dist\n",
    "    return self.normalizescores(mindistance,smallIsBetter=1)\n",
    "\n",
    "  def inboundlinkscore(self,rows):\n",
    "    uniqueurls=dict([(row[0],1) for row in rows])\n",
    "    inboundcount=dict([(u,self.con.execute('select count(*) from link where toid=%d' % u).fetchone()[0]) for u in uniqueurls])   \n",
    "    return self.normalizescores(inboundcount)\n",
    "\n",
    "  def linktextscore(self,rows,wordids):\n",
    "    linkscores=dict([(row[0],0) for row in rows])\n",
    "    for wordid in wordids:\n",
    "      cur=self.con.execute('select link.fromid,link.toid from linkwords,link where wordid=%d and linkwords.linkid=link.rowid' % wordid)\n",
    "      for (fromid,toid) in cur:\n",
    "        if toid in linkscores:\n",
    "          pr=self.con.execute('select score from pagerank where urlid=%d' % fromid).fetchone()[0]\n",
    "          linkscores[toid]+=pr\n",
    "    maxscore=max(linkscores.values())\n",
    "    normalizedscores=dict([(u,float(l)/maxscore) for (u,l) in linkscores.items()])\n",
    "    return normalizedscores\n",
    "\n",
    "  def pagerankscore(self,rows):\n",
    "    pageranks=dict([(row[0],self.con.execute('select score from pagerank where urlid=%d' % row[0]).fetchone()[0]) for row in rows])\n",
    "    maxrank=max(pageranks.values())\n",
    "    normalizedscores=dict([(u,float(l)/maxrank) for (u,l) in pageranks.items()])\n",
    "    return normalizedscores\n",
    "\n",
    "  def nnscore(self,rows,wordids):\n",
    "    # Get unique URL IDs as an ordered list\n",
    "    urlids=[urlid for urlid in dict([(row[0],1) for row in rows])]\n",
    "    nnres=mynet.getresult(wordids,urlids)\n",
    "    scores=dict([(urlids[i],nnres[i]) for i in range(len(urlids))])\n",
    "    return self.normalizescores(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html class=\"client-nojs\" lang=\"en\n"
     ]
    }
   ],
   "source": [
    "c = urllib2.urlopen('https://en.wikipedia.org/wiki/Web_crawler')\n",
    "contents=c.read()\n",
    "print contents[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing https://en.wikipedia.org/wiki/Web_crawler\n",
      "Indexing https://en.wikipedia.org/wiki/Web_crawler\n"
     ]
    }
   ],
   "source": [
    "pagelist=['https://en.wikipedia.org/wiki/Web_crawler']\n",
    "crawler = crawler('')\n",
    "crawler.crawl(pagelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
